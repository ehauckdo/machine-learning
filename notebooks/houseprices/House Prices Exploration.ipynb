{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices Exploration\n",
    "\n",
    "In this competition, we are trying to figure out the prices for which a bunch of houses have been sold.\n",
    "A lot of these features can be used out-of-the-box, so let's check what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking The Data\n",
    "\n",
    "Let's begin by first loading our training and test set and taking a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Id column won't be useful for building the model,\n",
    "# so let's make it the index of the dataframe\n",
    "train = pd.read_csv(\"input/train.csv\", index_col=0)\n",
    "test = pd.read_csv(\"input/test.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 80), (1459, 79))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "Id                                                                    \n",
       "1           60       RL           65     8450   Pave   NaN      Reg   \n",
       "2           20       RL           80     9600   Pave   NaN      Reg   \n",
       "3           60       RL           68    11250   Pave   NaN      IR1   \n",
       "4           70       RL           60     9550   Pave   NaN      IR1   \n",
       "5           60       RL           84    14260   Pave   NaN      IR1   \n",
       "\n",
       "   LandContour Utilities LotConfig    ...     PoolArea PoolQC Fence  \\\n",
       "Id                                    ...                             \n",
       "1          Lvl    AllPub    Inside    ...            0    NaN   NaN   \n",
       "2          Lvl    AllPub       FR2    ...            0    NaN   NaN   \n",
       "3          Lvl    AllPub    Inside    ...            0    NaN   NaN   \n",
       "4          Lvl    AllPub    Corner    ...            0    NaN   NaN   \n",
       "5          Lvl    AllPub       FR2    ...            0    NaN   NaN   \n",
       "\n",
       "   MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "Id                                                                         \n",
       "1          NaN       0      2    2008        WD         Normal     208500  \n",
       "2          NaN       0      5    2007        WD         Normal     181500  \n",
       "3          NaN       0      9    2008        WD         Normal     223500  \n",
       "4          NaN       0      2    2006        WD        Abnorml     140000  \n",
       "5          NaN       0     12    2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our dataset has 1460 and 1459 houses on our training and test set, respectively. We have a lot of categorial features in this dataset, so it's interesting to break them into dummy variables. To make it easier for us to create and manipulate these variables, let's join the training and test set, so that both will end up with the same columns. We can split them back after we've done all the data cleaning.\n",
    "\n",
    "Note: model evaluation is based on the log of SalePrice, so we will save it as log too. We need to change it back before exporting our submission file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 79)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's remove sale prices of our train before joining, and save them for later use\n",
    "Y_train = np.log(train.pop('SalePrice'))\n",
    "\n",
    "all_df = pd.concat((train, test), axis=0)\n",
    "all_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Transformations\n",
    "\n",
    "First thing to notice is that one of the features in the dataset, MSSubClass, is a categorical feature stored as a numeric one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['MSSubClass'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df['MSSubClass'] = all_df['MSSubClass'].apply(str)\n",
    "all_df['MSSubClass'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dummy variables\n",
    "\n",
    "Now that we have everything together, we need to generate dummy variables for our categorical features. Pandas can help us with the get_dummy_variable function. It generates dummy variables for a series or an entire dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>...</th>\n",
       "      <th>SaleType_ConLw</th>\n",
       "      <th>SaleType_New</th>\n",
       "      <th>SaleType_Oth</th>\n",
       "      <th>SaleType_WD</th>\n",
       "      <th>SaleCondition_Abnorml</th>\n",
       "      <th>SaleCondition_AdjLand</th>\n",
       "      <th>SaleCondition_Alloca</th>\n",
       "      <th>SaleCondition_Family</th>\n",
       "      <th>SaleCondition_Normal</th>\n",
       "      <th>SaleCondition_Partial</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>65</td>\n",
       "      <td>8450</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>196</td>\n",
       "      <td>706</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80</td>\n",
       "      <td>9600</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>0</td>\n",
       "      <td>978</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>68</td>\n",
       "      <td>11250</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>162</td>\n",
       "      <td>486</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>9550</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>84</td>\n",
       "      <td>14260</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>350</td>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 303 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  YearRemodAdd  \\\n",
       "Id                                                                            \n",
       "1            65     8450            7            5       2003          2003   \n",
       "2            80     9600            6            8       1976          1976   \n",
       "3            68    11250            7            5       2001          2002   \n",
       "4            60     9550            7            5       1915          1970   \n",
       "5            84    14260            8            5       2000          2000   \n",
       "\n",
       "    MasVnrArea  BsmtFinSF1  BsmtFinSF2  BsmtUnfSF          ...            \\\n",
       "Id                                                         ...             \n",
       "1          196         706           0        150          ...             \n",
       "2            0         978           0        284          ...             \n",
       "3          162         486           0        434          ...             \n",
       "4            0         216           0        540          ...             \n",
       "5          350         655           0        490          ...             \n",
       "\n",
       "    SaleType_ConLw  SaleType_New  SaleType_Oth  SaleType_WD  \\\n",
       "Id                                                            \n",
       "1                0             0             0            1   \n",
       "2                0             0             0            1   \n",
       "3                0             0             0            1   \n",
       "4                0             0             0            1   \n",
       "5                0             0             0            1   \n",
       "\n",
       "    SaleCondition_Abnorml  SaleCondition_AdjLand  SaleCondition_Alloca  \\\n",
       "Id                                                                       \n",
       "1                       0                      0                     0   \n",
       "2                       0                      0                     0   \n",
       "3                       0                      0                     0   \n",
       "4                       1                      0                     0   \n",
       "5                       0                      0                     0   \n",
       "\n",
       "    SaleCondition_Family  SaleCondition_Normal  SaleCondition_Partial  \n",
       "Id                                                                     \n",
       "1                      0                     1                      0  \n",
       "2                      0                     1                      0  \n",
       "3                      0                     1                      0  \n",
       "4                      0                     0                      0  \n",
       "5                      0                     1                      0  \n",
       "\n",
       "[5 rows x 303 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dummy_df = pd.get_dummies(all_df)\n",
    "\n",
    "# let's check if everything went alright\n",
    "all_dummy_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Column order looks a little messy. They are in alphabetical order now, but that shouldn't be a problem. But we do have a lot of columns, which is possibly a bigger deal to be worried with.\n",
    "\n",
    "Now that we have our dummy variables, we should check for NaN values in our columns. For the categorical features, pandas' get_dummy_variables takes care of this for us. For example, for a feature X which could get values a, b or c, if an individual has NaN for X, all 3 resulting columns (X_a, X_b, X_c) will be assigned 0. (Note: it can be useful to create a new column for missing values, e.g., X_missing, because not being assigned to any label could be informative; we should get back at this later)\n",
    "\n",
    "## Missing Values\n",
    "\n",
    "Since there's no missing values for our categorical features, we just need to check the numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LotFrontage             486\n",
       "GarageYrBlt             159\n",
       "MasVnrArea               23\n",
       "BsmtHalfBath              2\n",
       "BsmtFullBath              2\n",
       "BsmtFinSF2                1\n",
       "GarageCars                1\n",
       "TotalBsmtSF               1\n",
       "BsmtUnfSF                 1\n",
       "GarageArea                1\n",
       "BsmtFinSF1                1\n",
       "Condition1_Artery         0\n",
       "Condition2_Feedr          0\n",
       "Condition2_Artery         0\n",
       "Neighborhood_Somerst      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dummy_df.isnull().sum().sort_values(ascending=False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to take care of these missing values. For now, let's just assign the mean for all missing values. There's probably better ways to deal with that, but let's just do the easy way now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SaleCondition_Partial    0\n",
       "Neighborhood_NridgHt     0\n",
       "Neighborhood_SWISU       0\n",
       "Neighborhood_Sawyer      0\n",
       "Neighborhood_SawyerW     0\n",
       "Neighborhood_Somerst     0\n",
       "Neighborhood_StoneBr     0\n",
       "Neighborhood_Timber      0\n",
       "Neighborhood_Veenker     0\n",
       "Condition1_Artery        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_columns = all_dummy_df.mean()\n",
    "all_dummy_df = all_dummy_df.fillna(mean_columns)\n",
    "\n",
    "all_dummy_df.isnull().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize numerical features\n",
    "\n",
    "Since we are going to do regression on this dataset, it's recommended to standardize our numerical features. So we are going to get all numerical features, subtract by the mean and divide by the standard deviation. This will make them all scaled similarly.\n",
    "\n",
    "First let's break our dataset into train and test again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 303), (1459, 303))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_train_df = all_dummy_df.loc[train.index]\n",
    "dummy_test_df = all_dummy_df.loc[test.index]\n",
    "\n",
    "dummy_train_df.shape, dummy_test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get our numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'LotFrontage', u'LotArea', u'OverallQual', u'OverallCond',\n",
       "       u'YearBuilt', u'YearRemodAdd', u'MasVnrArea', u'BsmtFinSF1',\n",
       "       u'BsmtFinSF2', u'BsmtUnfSF', u'TotalBsmtSF', u'1stFlrSF', u'2ndFlrSF',\n",
       "       u'LowQualFinSF', u'GrLivArea', u'BsmtFullBath', u'BsmtHalfBath',\n",
       "       u'FullBath', u'HalfBath', u'BedroomAbvGr', u'KitchenAbvGr',\n",
       "       u'TotRmsAbvGrd', u'Fireplaces', u'GarageYrBlt', u'GarageCars',\n",
       "       u'GarageArea', u'WoodDeckSF', u'OpenPorchSF', u'EnclosedPorch',\n",
       "       u'3SsnPorch', u'ScreenPorch', u'PoolArea', u'MiscVal', u'MoSold',\n",
       "       u'YrSold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = all_df.columns[all_df.dtypes != 'object']\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the normalization both on our train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_numeric_col_means = dummy_train_df.loc[:, numeric_cols].mean()\n",
    "train_numeric_col_std = dummy_train_df.loc[:, numeric_cols].std()\n",
    "\n",
    "dummy_train_df.loc[:, numeric_cols] = (dummy_train_df.loc[:, numeric_cols] - train_numeric_col_means) / train_numeric_col_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_numeric_col_means = dummy_test_df.loc[:, numeric_cols].mean()\n",
    "test_numeric_col_std = dummy_test_df.loc[:, numeric_cols].std()\n",
    "\n",
    "dummy_test_df.loc[:, numeric_cols] = (dummy_test_df.loc[:, numeric_cols] - test_numeric_col_means) / test_numeric_col_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the histogram of a variable to see if everything went alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<matplotlib.axes._subplots.AxesSubplot at 0x7f26bc5f0ed0>,\n",
       " <matplotlib.axes._subplots.AxesSubplot at 0x7f26bc5f0ed0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8uEFyQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26bc5f0ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_train_df['GrLivArea'].hist(), dummy_test_df['GrLivArea'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some outliers but we are going to just leave it like that for now.\n",
    "\n",
    "And this is all the data preparation we need. There's still a lot of things that could be done, but let just proceed for building the model and checking how well we can do with this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building The Model\n",
    "\n",
    "Let's try building some models now. I'll just try some available in sklearn library and see what works best. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = list(dummy_train_df.describe().columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have everything prepared, we are going to follow some steps for each regressor we are going to use.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1 - find the most relevant/important features for the model\\* <br />\n",
    "2 - tune the hyper-parameters <br />\n",
    "3 - cross validate the model on our training data \n",
    "\n",
    "\\*This step will vary depending on the regressor. Tree-based regressor have a built-in feature_importances field which we can use to get the most relevant features. For other approach-based regressors, we are going to use the SelectKBest class to get the most relevant features of our dataset for doing regression on the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Finding the best features\n",
    "\n",
    "As mentioned, the way to select features depends on the regressor. Here, we are going to have two approaches, each one defining a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we are going to use this function for tree-based regressors\n",
    "def get_best_threshold(regressor, X, Y, initial_threshold=0.1, limit_features=100, threshold_increase=-0.0001, title=\"\"):\n",
    "\n",
    "    base = np.sqrt(-cross_val_score(regressor, X, Y, cv=10, scoring='neg_mean_squared_error'))\n",
    "    sfm = SelectFromModel(regressor, threshold=initial_threshold)\n",
    "\n",
    "    n_features = sfm.fit_transform(X, Y).shape[1]\n",
    "    last_n_features = n_features\n",
    "    threshold_results = []\n",
    "    \n",
    "    f = open(\"output/{0}.txt\".format(title),\"w\")\n",
    "    f.write(\"Result: {0}, N_features: All\\n\".format(np.mean(base)))\n",
    "    \n",
    "    while n_features < limit_features:\n",
    "        sfm.threshold += threshold_increase\n",
    "        X_new = sfm.fit_transform(X, Y)\n",
    "        n_features = X_new.shape[1]\n",
    "\n",
    "        if n_features > last_n_features:\n",
    "            \n",
    "            last_n_features = n_features\n",
    "            selected_features_score = np.sqrt(-cross_val_score(regressor, X_new, Y, cv=10, scoring='neg_mean_squared_error'))\n",
    "            f.write(\"Result: {0}, Threshold: {1}, N_features: {2}\\n\".format(np.mean(selected_features_score), sfm.threshold, n_features))\n",
    "            threshold_results.append((sfm.threshold, np.mean(selected_features_score)))\n",
    "            \n",
    "    better_threshold = min(threshold_results, key = itemgetter(1))[0]\n",
    "    f.close()\n",
    "    return better_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we are going to use this function for non-tree-based regressors\n",
    "def get_best_features(regressor, X, Y, limit_features=100, feature_increase=3, title=\"\"):\n",
    "\n",
    "    k_features = 10\n",
    "    k_features_results = []\n",
    "    f = open(\"output/{0}.txt\".format(title),\"w\")\n",
    "\n",
    "    # use SelectKBest to iterate over important features\n",
    "    while k_features < limit_features:\n",
    "        selector = SelectKBest(f_regression, k=k_features)\n",
    "        X_new = selector.fit_transform(X, Y)\n",
    "        selected_features_score = np.sqrt(-cross_val_score(regressor, X_new, Y, cv=10, scoring='neg_mean_squared_error'))\n",
    "        f.write(\"Result: {0},  N_features: {1}\\n\".format(np.mean(selected_features_score), k_features))\n",
    "        k_features_results.append((k_features, np.mean(selected_features_score)))\n",
    "        k_features += feature_increase\n",
    "        \n",
    "    f.close()\n",
    "    best_k_features = min(k_features_results, key = itemgetter(1))[0]\n",
    "    return best_k_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Tuning the hyper-parameters\n",
    "\n",
    "Here we are going to use sklearn's GridSearchCV to exhaustively search a for the best params in a dictionary. You need to pass it values to be tested. If you don't don't know which range of values to use for a parameter, an idea is to check the default value, and work from there. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hyper_parameters(alg, parameters, X, Y, title=\"GridSearch\"):\n",
    "    \n",
    "    f = open(\"output/{0}.txt\".format(title),\"w\")\n",
    "    gs = GridSearchCV(alg, parameters, n_jobs=3, cv=5)                \n",
    "    gs.fit(X, Y)\n",
    "    for param in gs.best_params_.keys():\n",
    "        f.write(\"Param: {0},  Value: {1}\\n\".format(param, gs.best_params_[param]))\n",
    "    f.close()\n",
    "        \n",
    "    return gs.best_params_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Cross-validating\n",
    "\n",
    "We don't have any code for this, it will be done after we build the model. One important thing to notice is the number of CV folds to use, and the scorer parameter. Here we are going to use the 'neg_mean_squared_error' because this is the scoring function which Kaggle will validate your results, so it's good to have similar score range while cross-validating, so we can have an idea how well we are going to do on the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# some of these parameters have already been tuned\n",
    "# we are going to declare them here so we don't have\n",
    "# grid search everytime we run this notebook\n",
    "\n",
    "RF_data = {\n",
    "    #all features                   #selected features\n",
    "    'n_estimators':       300,     'n_estimators_all':      300,   \n",
    "    'max_features':       0.5,     'max_features_all':       0.2,  \n",
    "    'min_samples_leaf':   1,       'min_samples_leaf_all':   1,\n",
    "    'min_samples_split':  1,       'min_samples_split_all':  4,\n",
    "    'max_depth':          None,    'max_depth_all':          None,\n",
    "    'best_threshold':     0.0036\n",
    "}\n",
    "\n",
    "GB_data = {\n",
    "    #all features                  #selected features\n",
    "    'n_estimators':       300,     'n_estimators_all':       300,    \n",
    "    'max_features':       'sqrt',  'max_features_all':       'sqrt',     \n",
    "    'min_samples_leaf':   1,       'min_samples_leaf_all':   1,\n",
    "    'min_samples_split':  1,       'min_samples_split_all':  1,\n",
    "    'max_depth':          3,       'max_depth_all':          3,         \n",
    "    'learning_rate':      0.1,     'learning_rate_all':      0.1, \n",
    "    'best_threshold':     0.0024\n",
    "}\n",
    "\n",
    "RR_data = {\n",
    "    'alpha': 0.1,                   'alpha_all': 0.1,          \n",
    "    'best_n_features': 287,  \n",
    "}\n",
    "\n",
    "XGB_data = {\n",
    "    'n_estimators':       100,\n",
    "    'learning_rate':      0.1,\n",
    "    'best_n_features' :   241,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble.forest import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13553416885231812, 0.13637899811891435)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# First step: get the best features for our train and test\n",
    "# this step was previously done so we can skip it\n",
    "#best_threshold = get_best_threshold(alg, dummy_train_df[all_features], Y_train, title=\"RF_threshold\")\n",
    "#RF_data['best_threshold'] = best_threshold\n",
    "\n",
    "sfm = SelectFromModel(alg, threshold=RF_data['best_threshold'])\n",
    "X_train_new = sfm.fit_transform(dummy_train_df[all_features], Y_train)\n",
    "X_test_new = sfm.transform(dummy_test_df[all_features])\n",
    "\n",
    "# Second step: grid search for the best params\n",
    "# this step was previously done so we can skip it\n",
    "#parameters = {\n",
    "#    'n_estimators' : [5, 10, 25, 40, 65, 80, 100, 150, 200, 300],\n",
    "#    'max_features' : [\"auto\", \"sqrt\", \"log2\", 0.2, 0.5],\n",
    "#    'min_samples_leaf' : [1, 5, 10, 25, 50],\n",
    "#    'min_samples_split' : [1, 2, 4, 8] ,\n",
    "#    'max_depth': [None, 3, 5, 10]\n",
    "#    }\n",
    "\n",
    "#bp = get_hyper_parameters(alg, parameters, X_train_new, Y_train, \"RF_grid_selected\")\n",
    "#bp2 = get_hyper_parameters(alg, parameters, dummy_train_df[all_features], Y_train, \"RF_grid_all\")\n",
    "#let's store this information for later use\n",
    "#RF_data['n_estimators'] = bp['n_estimators']\n",
    "#RF_data['max_features'] = bp['max_features']\n",
    "#RF_data['min_samples_split'] = bp['min_samples_split']\n",
    "#RF_data['min_samples_leaf'] = bp['min_samples_leaf']\n",
    "#RF_data['max_depth'] = bp['max_depth']\n",
    "\n",
    "# Third step: cross-validate, here we compare the cross-validated\n",
    "# scores both using all the features and just the selected features\n",
    "regr_selected = RandomForestRegressor(random_state=1, n_estimators=RF_data['n_estimators'], max_features=RF_data['max_features'],\n",
    "                            min_samples_split=RF_data['min_samples_split'], min_samples_leaf=RF_data['min_samples_leaf'],\n",
    "                            max_depth=RF_data['max_depth'])\n",
    "regr_all = RandomForestRegressor(random_state=1, n_estimators=RF_data['n_estimators_all'], \n",
    "                            max_features=RF_data['max_features_all'], min_samples_split=RF_data['min_samples_split_all'], \n",
    "                            min_samples_leaf=RF_data['min_samples_leaf_all'], max_depth=RF_data['max_depth_all'])\n",
    "\n",
    "selected_features_score = np.sqrt(-cross_val_score(regr_selected, X_train_new, Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "all_features_score = np.sqrt(-cross_val_score(regr_all, dummy_train_df[all_features], Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "\n",
    "selected_features_score = np.mean(selected_features_score)\n",
    "all_features_score = np.mean(all_features_score)\n",
    "\n",
    "# Last step: run our model and generate submission files\n",
    "submission =  pd.DataFrame()\n",
    "submission[\"Id\"] = test.index\n",
    "\n",
    "regr_all.fit(dummy_train_df[all_features], Y_train)\n",
    "predictions = regr_all.predict(dummy_test_df)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/rf_whole_submission.csv\", index=False)\n",
    "\n",
    "regr_selected.fit(X_train_new, Y_train)\n",
    "predictions = regr_selected.predict(X_test_new)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/rf_subset_submission.csv\", index=False)\n",
    "\n",
    "np.mean(selected_features_score), np.mean(all_features_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's seems that selecting the best features gives us just slightly better results than just using all the generated features. As expected the impact is not so big for tree based approaches, but it should be worth repeating this test for regressors with different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12010130279026257, 0.12210879625872809)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = GradientBoostingRegressor(random_state=1)\n",
    "\n",
    "# First step: get the best features for our train and test\n",
    "# this step was previously done so we can skip it\n",
    "#best_threshold = get_best_threshold(alg, dummy_train_df[all_features], Y_train, title=\"GB_threshold\")\n",
    "#GB_data['best_threshold'] = best_threshold\n",
    "\n",
    "sfm = SelectFromModel(alg, threshold=GB_data['best_threshold'])\n",
    "X_train_new = sfm.fit_transform(dummy_train_df[all_features], Y_train)\n",
    "X_test_new = sfm.transform(dummy_test_df[all_features])\n",
    "\n",
    "# Second step: grid search for the best params\n",
    "# this step was previously done so we can skip it\n",
    "#parameters = {\n",
    "#    'n_estimators' : [5, 10, 25, 40, 65, 80, 100, 150, 200, 300],\n",
    "#    'max_features' : [\"auto\", \"sqrt\", \"log2\", 0.2, 0.5],\n",
    "#    'min_samples_leaf' : [1, 5, 10, 25, 50],\n",
    "#    'min_samples_split' : [1, 2, 4, 8],\n",
    "#    'max_depth': [None, 3, 5, 10],\n",
    "#    'learning_rate' : [0.05, 0.08, 0.1, 0.15, 0.2]\n",
    "#    }\n",
    "#\n",
    "#bp = get_hyper_parameters(alg, parameters, X_train_new, Y_train, \"GB_grid_selected\")\n",
    "#bp = get_hyper_parameters(alg, parameters, X_train_new, Y_train, \"GB_grid_all\")\n",
    "#let's store this information for later use\n",
    "#GB_data['n_estimators'] = bp['n_estimators']\n",
    "#GB_data['max_features'] = bp['max_features']\n",
    "#GB_data['min_samples_split'] = bp['min_samples_split']\n",
    "#GB_data['min_samples_leaf'] = bp['min_samples_leaf']\n",
    "#GB_data['max_depth'] = bp['max_depth']\n",
    "#GB_data['learning_rate'] = bp['learning_rate']\n",
    "\n",
    "# Third step: cross-validate, here we compare the cross-validated\n",
    "# scores both using all the features and just the selected features\n",
    "regr_selected = GradientBoostingRegressor(random_state=1, n_estimators=GB_data['n_estimators'], \n",
    "                                max_features=GB_data['max_features'], min_samples_split=GB_data['min_samples_split'], \n",
    "                                min_samples_leaf=GB_data['min_samples_leaf'], learning_rate=GB_data['learning_rate'])\n",
    "regr_all = GradientBoostingRegressor(random_state=1, n_estimators=GB_data['n_estimators_all'], \n",
    "                                max_features=GB_data['max_features_all'], \n",
    "                                min_samples_split=GB_data['min_samples_split_all'], \n",
    "                                min_samples_leaf=GB_data['min_samples_leaf_all'], \n",
    "                                learning_rate=GB_data['learning_rate_all'])\n",
    "\n",
    "selected_features_score = np.sqrt(-cross_val_score(regr_selected, X_train_new, Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "all_features_score = np.sqrt(-cross_val_score(regr_all, dummy_train_df[all_features], Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Last step: run our model and generate submission files\n",
    "submission =  pd.DataFrame()\n",
    "submission[\"Id\"] = test.index\n",
    "\n",
    "regr_all.fit(dummy_train_df[all_features], Y_train)\n",
    "predictions = regr_all.predict(dummy_test_df)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/gb_whole_submission.csv\", index=False)\n",
    "\n",
    "regr_selected.fit(X_train_new, Y_train)\n",
    "predictions = regr_selected.predict(X_test_new)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/gb_subset_submission.csv\", index=False)\n",
    "\n",
    "np.mean(selected_features_score), np.mean(all_features_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.15605243026137602, 0.14002227490842117)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = Ridge(random_state=1, normalize=True)\n",
    "\n",
    "# First step: get the best features for our train and test\n",
    "# this step was previously done so we can skip it\n",
    "#best_n_features = get_best_features(alg, dummy_train_df[all_features], Y_train, len(all_features)+1, 1, title=\"RR_threshold\")\n",
    "#RR_data['best_n_features'] = best_n_features\n",
    "\n",
    "selector = SelectKBest(f_regression, k=RR_data['best_n_features'])\n",
    "X_train_new = selector.fit_transform(dummy_train_df[all_features], Y_train)\n",
    "X_test_new = selector.transform(dummy_test_df[all_features])\n",
    "\n",
    "# Second step: grid search for the best params\n",
    "# this step was previously done so we can skip it\n",
    "parameters = {\n",
    "'alpha' : [1,0.1,0.01,0.001,0.0001,0]\n",
    "}\n",
    "\n",
    "#bp = get_hyper_parameters(alg, parameters, X_train_new, Y_train, \"RR_grid_selected\")\n",
    "#bp2 = get_hyper_parameters(alg, parameters, dummy_train_df[all_features], Y_train, \"RR_grid_all\")\n",
    "\n",
    "\n",
    "# Third step: cross-validate, here we compare the cross-validated\n",
    "# scores both using all the features and just the selected features\n",
    "regr_selected = Ridge(random_state=1, alpha=RR_data['alpha'])\n",
    "regr_all = alg = Ridge(random_state=1, normalize=True, alpha=RR_data['alpha_all'])\n",
    "          \n",
    "selected_features_score = np.sqrt(-cross_val_score(regr_selected, X_train_new, Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "all_features_score = np.sqrt(-cross_val_score(regr_all, dummy_train_df[all_features], Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Last step: run our model and generate submission files\n",
    "submission =  pd.DataFrame()\n",
    "submission[\"Id\"] = test.index\n",
    "\n",
    "regr_all.fit(dummy_train_df[all_features], Y_train)\n",
    "predictions = regr_all.predict(dummy_test_df)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/ridge_whole_submission.csv\", index=False)\n",
    "\n",
    "regr_selected.fit(X_train_new, Y_train)\n",
    "predictions = regr_selected.predict(X_test_new)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/ridge_subset_submission.csv\", index=False)\n",
    "\n",
    "np.mean(selected_features_score), np.mean(all_features_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13077265899511237, 0.13112753205742261)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alg = xgb.XGBRegressor(seed=0, nthread=3, n_estimators=XGB_data['n_estimators'],\n",
    "                                learning_rate=XGB_data['learning_rate'])\n",
    "\n",
    "# First step: get the best features for our train and test\n",
    "# this step was previously done so we can skip it\n",
    "#best_threshold = get_best_features(alg, dummy_train_df[all_features], Y_train, len(all_features)+1, title=\"XGB_threshold\")\n",
    "#XGB_data['best_threshold'] = best_threshold\n",
    "\n",
    "selector = SelectKBest(f_regression, k=XGB_data['best_n_features'])\n",
    "X_train_new = selector.fit_transform(dummy_train_df[all_features], Y_train)\n",
    "X_test_new = selector.transform(dummy_test_df[all_features])\n",
    "\n",
    "# Second step: grid search for the best params\n",
    "# this step was previously done so we can skip it\n",
    "#parameters = {\n",
    "#    'n_estimators' : [5, 10, 25, 40, 65, 80, 100, 150, 200, 300],\n",
    "#    'min_samples_leaf' : [1, 5, 10, 25, 50],\n",
    "#    'min_samples_split' : [1, 2, 4, 8],\n",
    "#    'max_depth': [None, 3, 5, 10],\n",
    "#    'learning_rate' : [0.05, 0.08, 0.1, 0.15, 0.2]\n",
    "#    }\n",
    "#\n",
    "#bp = get_hyper_parameters(alg, parameters, X_train_new, Y_train, \"XGB_grid_selected\")\n",
    "#bp = get_hyper_parameters(alg, parameters,  dummy_train_df[all_features], Y_train, \"XGB_grid_all\")\n",
    "\n",
    "# Third step: cross-validate, here we compare the cross-validated\n",
    "# scores both using all the features and just the selected features\n",
    "regr = xgb.XGBRegressor(seed=0, nthread=3, n_estimators=XGB_data['n_estimators'],\n",
    "                                learning_rate=XGB_data['learning_rate'])\n",
    "\n",
    "selected_features_score = np.sqrt(-cross_val_score(regr, X_train_new, Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "all_features_score = np.sqrt(-cross_val_score(regr, dummy_train_df[all_features], Y_train, cv=8, scoring='neg_mean_squared_error'))\n",
    "\n",
    "# Last step: run our model and generate submission files\n",
    "submission =  pd.DataFrame()\n",
    "submission[\"Id\"] = test.index\n",
    "\n",
    "regr.fit(dummy_train_df[all_features], Y_train)\n",
    "predictions = regr.predict(dummy_test_df)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/xgb_whole_submission.csv\", index=False)\n",
    "\n",
    "regr.fit(X_train_new, Y_train)\n",
    "predictions = regr.predict(X_test_new)\n",
    "submission[\"SalePrice\"] = np.exp(predictions)\n",
    "submission.to_csv(\"output/xgb_subset_submission.csv\", index=False)\n",
    "\n",
    "np.mean(selected_features_score), np.mean(all_features_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#alg = MLPRegressor(random_state=1)\n",
    "#\n",
    "#best_features = get_best_features(alg, dummy_train_df[all_features], Y_train, 100, 3)\n",
    "#          \n",
    "#selector = SelectKBest(f_regression, k=best_features)\n",
    "#X_train_new = selector.fit_transform(dummy_train_df[all_features], Y_train)\n",
    "#X_test_new = selector.transform(dummy_test_df[all_features])\n",
    "#          \n",
    "#now let's try comparing the selected KBest features with all features\n",
    "#selected_features_score = np.sqrt(-cross_val_score(alg, X_train_new, Y_train, cv=3, scoring='neg_mean_squared_error'))\n",
    "#all_features_score = np.sqrt(-cross_val_score(alg, dummy_train_df[all_features], Y_train, cv=3, scoring='neg_mean_squared_error'))\n",
    "#\n",
    "#np.mean(selected_features_score), np.mean(all_features_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Multi-Layer Perceptron it seems that having many features just make it worse. Using only the overall quality of the house as a feature wields the best result, which is far from good by the way. For now, let's just be done with this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#alg = SVR()\n",
    "#alg = SVR(epsilon=0, C=2, gamma=3.0517578125e-05)\n",
    "#\n",
    "#best_features = get_best_features(alg, dummy_train_df[all_features], Y_train, 100, 3)\n",
    "#          \n",
    "#selector = SelectKBest(f_regression, k=best_features)\n",
    "#X_train_new = selector.fit_transform(dummy_train_df[all_features], Y_train)\n",
    "#X_test_new = selector.transform(dummy_test_df[all_features])\n",
    "#\n",
    "#parameters = {   \n",
    "#    'C' : [2**i for i in range(-5,15, 2)], # 2^-5, 2^-3,..., 2^15\n",
    "#    'gamma' : [2**i for i in range(-15,3, 2)], # 2^-15, 2^-13,..., 2^3\n",
    "#    'epsilon' : [0, 0.01, 0.1, 0.5, 1, 2, 4]\n",
    "#}    \n",
    "#\n",
    "#bp = get_hyper_parameters(alg, parameters, X_train_new, Y_train)\n",
    "#alg = SVR(epsilon=bp['epsilon'], C=bp['C'], gamma=bp['gamma'])\n",
    "#    \n",
    "#now let's try comparing the selected KBest features with all features\n",
    "#selected_features_score = np.sqrt(-cross_val_score(alg, X_train_new, Y_train, cv=3, scoring='neg_mean_squared_error'))\n",
    "#all_features_score = np.sqrt(-cross_val_score(alg, dummy_train_df[all_features], Y_train, cv=3, scoring='neg_mean_squared_error'))\n",
    "#\n",
    "#np.mean(selected_features_score), np.mean(all_features_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same reasoning for MLP goes for SVR, using many features is not very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we will pass a list of regressors and this function \n",
    "# will perform the ensemble for us\n",
    "def perform_ensemble(regressors, train, target, test):\n",
    "    total_predictions = []\n",
    "    for alg in regressors:\n",
    "        alg.fit(train, target)\n",
    "        prediction = algorithm.predict(test)\n",
    "        total_predictions.append(prediction)\n",
    "\n",
    "    # average scores\n",
    "    sum_predictions = 0\n",
    "    for prediction in total_predictions:\n",
    "        sum_predictions += prediction\n",
    "    final_prediction = sum_predictions / len(total_predictions)\n",
    "    \n",
    "    return final_prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reference Links\n",
    "\n",
    "[Ames, Iowa: Alternative to the Boston Housing Data as an End of Semester Regression Project (read this for insights on the dataset)](https://ww2.amstat.org/publications/jse/v19n3/decock.pdf)\n",
    "\n",
    "[How to Tune Algorithm Parameters with Scikit-Learn](http://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/)\n",
    "\n",
    "[Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/)\n",
    "\n",
    "[Ridge and Lasso tutorial](https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/)\n",
    "\n",
    "[Tuning the parameters of your Random Forest model](https://www.analyticsvidhya.com/blog/2015/06/tuning-random-forest-model/)\n",
    "\n",
    "[Ensemble Modeling: Stack Model Example](https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/discussion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
